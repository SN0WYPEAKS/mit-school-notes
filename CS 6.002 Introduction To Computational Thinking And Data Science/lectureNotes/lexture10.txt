lecture notes:
    fit a line:
        model1 = pylab.polyfit(xVals, yVals, 1)
        pylab.plot(xVals, pylab.polyval(model1, xVals),
                   'r--', label = 'Linear Model')
    remember that pylab.polyfit will find parameters of best fitting
    polynomial of described order:
        in this case (with argument n = 1), find the values of a and b,
        such that y = ax + b best matches the observe yVals
    remember that pylab.polyval will generate predicted yVals given
    parameters of model
    can we get a tighter fit?:
        what if we try fitting higher order polynomials to the data?:
            does this give us a better fit?
        how would we measure that?:
            in absence of other information
            (e.g., theoretical insights into order of model),
            R²(coefficient of determination)
                         ∑ᵢ(yᵢ - pᵢ)²   (Error in estimates)
                R² = 1 - -----------
                         ∑ᵢ(yᵢ - µ)²    (variability in measured data)
                Yᵢ:
                    are measured values
                Pᵢ:
                    are predicted values
                µ:
                    is mean(average) of measured values
            gives us decent measure of the tightness of the model fit
    motivation for mystery data - parabola(a curve where any point is a at an equal distance from:
                                           a fixed point(the focus), and a fixed straight line (the directrix)):
        trajectory of a particel under the influence of a
        uniform gravitational field (e.g. Halley's Comet)
        position of center of mass of a football pass
        design of a load-bearing arch
    how mystery data was generated:
        def genNoisyParabolicData(a, b, c, xVals, fName):
            yVals = []
            for x in xVals:
                theoreticalVal = a*x**2 + b*x + c
                yVals.append(theoreticalVal + random.gauss(0, 35))
            f = open(fName, 'w')
            f.write('x      y\n')
            for i in range(len(yVals)):
                f.write(str(yVals[i]) + ' ' + str(xVals[i]) + '\n')
            f.close()
        #parameters for generating data
        xVals = range(-10, 11, 1)
        a, b, c = 3, 0, 0
        genNoisyParabolicData(a, b, c, xVals, 'Mystery Data.txt')
    if data was generated by quadratic, why was 16ᵗʰ order polynomial the "best" fit?
    let's look at two data sets:
        degrees = (2, 4, 8, 16)

        random.seed(0)
        xVals1, yVals1 = getData('Dataset 1.txt')
        models1 = genFits(xVals1, yVals1, degrees)
        testFits(models1, degrees, xVals1, yVals1, 'DataSet 1.txt')

        pylab.figure()
        xVals2, yVals2 = getData(Dataset 2.txt')
        models2 = genFits(xVals2, yVals2, degrees)
        testFits(models2, degrees, xVals2, yVals2, 'Dataset 2.txt')
    cross validate:
        generate models using one dataset, and then test them on another dataset:
            use models for dataset 1 to predict points for dataset 2
            use models for dataset 2 to predict points for dataset 1
        test code:  
            pylab.figure()
            testFits(models1, degrees, xVals2, yVals2, 'DataSet 2/Model 1')
            pylab.figure()
            testFits(models2, degrees, xVals1, yVals1, 'DataSet 1/Model 2')
    overfitting:
        an overfit model is one that is too complicated for your data set.
        when this happens, the regression model becomes tailored to fit the
        quirks and random noise in your specific sample rather than reflecting the overall population
        to avoid this, train your model on one set, and test it on another.
    fitting a quadratic to a perfect line:
        xVals = (0,1,2,3)
        yVals = xVals
        pylab.plot(xVals, yVals, label = 'Actual values')

        a,b,c = pylab.polyfit(xVals, yVals, 2)
        print('a =', round(a, 4), 'b =', round(b, 4), 'c =', round(c, 4))

        estYVals = pylab.polyval((a,b,c), xVals)
        pylab.plot(xVals, estYVals, 'r--', label ='Predicted values')
        print('R-squared = ', rSquared(yVals, estYVals))
    answer found in the above example:
        y = ax² + bx + c
        y = 0x² + 1x + 0
        y = x
        R-squared = 1.0
    predicted another point using the same model:
        xVals = xVals + (20,)
        yVals = xVals
        pylab.plot(xVals, yVals, label = 'Actual values')
        estYVals = pylab.polyval((a,b,c), xVals)
        pylab.plot(xVals, estYVals, 'r--', label = 'Predicted values')
        print('R-squared = ', rSquared(yVals, estYVals))
    answer found:
        R-squared = 1.0
    simulate a small measurement error:
        xVals = (0,1,2,3)
        yVals = (0,1,2,3.1)
        pylab.plot(xVals, yVals, label = 'Actual values')
        model = pylab.polyfit(xVals, yVals, 2)
        print(model)
        estYVals = pylab.polyval(model, xVals)
        pylab.plot(xVals, estYVals, 'r--', label = 'Predicted values')
        print('R-squared = ', rSquared(yVals, estYVals))
    answer found:
        y = ax² + bx + c
        y = .025x² + .955x + .005
        R-squared = 0.9994
    predicted another point using the same model with the small measurement error:
        xVals = xVals + (20,)
        yVals = xVals
        pylab.plot(xVals, yVals, label = 'Actual values')
        estYVals = pylab.polyval((a,b,c), xVals)
        pylab.plot(xVals, estYVals, 'r--', label = 'Predicted values')
        print('R-squared = ', rSquared(yVals, estYVals))
    answer found:
        R-squared = 0.7026
        way off
    suppose we had used a first-degree fit:
        model = pylab.polyfit(xVals, yVals, 1)
    answer found:
        R-squared = 0.9988
    comparing first and second degree fits:
        predictive ability of first order fit much better than second order fit
    if i pick an overly complex model i risk overfitting
    an overly simple model won't explain the data well
    there must be a balance. things should be as simple as possible, but not too simple
    returning where we started:
        quadratic fit tighter
        but remember hooke
        unless we believe theory is wrong, that should guide us
        model holds until reach elastic limit of spring
        should probably fit different models to different segments of data
        can visualize as search process - find best place to break into two parts,
        such that both linear segments have high R² fits
    leave-one-out cross validation:
        Let D be the original data set

        testResults = []
        for i in range(len(D)):
            training = D[:].pop(i)
            model = buildModel(training)
            testResults.append(test(model, D[i]))
        Average testResults

        take the data set (or a copy) and drop out one of the samples
        first leave out the first, then the second, third and so on
        then average the test results
        works on smaller data sets

    for larger data sets:
        k-fold:
            divide the data sets into equal sized chunks
            then leave out the first chunk test the rest
            then the second and so on
            average the results
    
    repeated random sampling:
        Let D be the original data set
            n be the number of random samples
                usually n between 20% and 50%
            k be number of trials

        testResults = []
        for i in range(k):
            randomly select n elements for testSet,
                keep rest for training
            model = buildModel(training)
            testResults.append(test(model, testSet))
        Average testResults

    an example, temperature by year:
        task:
            model how the mean daily high temperature in the u.s. varied from 1961 through 2015
        get means for each year and plot them
        randomly divide data in half n times:
            for each dimensionality to be tried:
                train on one half of data
                test on other half
                record r-squared on test data
        report mean r-squared for each dimensionality
    a boring class that reads in temperature data:
        class tempDatum(object):
            def __init__(self, s):
                info = s.split(',')
                self.high = float(info[1])
                self.year = int(info[2][0:4])
            def getHigh(self):
                return self.high
            def getYear(self):
                return self.year
    read temp data from a file:
        def getTempData():
            inFile = open('temperatures.csv')
            data = []
            for l in inFile:
                data.append(tempDatum(l))
            return data
    get the mean high temperature for each year:
        def getYearlyMeans(data):
            years = {}
            for d in data:
                try:
                    years[d.getYear()].append(d.getHigh())
                except:
                    years[d.getYear()] = [d.getHigh()]
            for y in years:
                years[y] = sum(years[y])/len(years[y])
            return years
    get and plot data:
        data = getTempData()
        years = getYearlyMeans(data)
        xVals, yVals = [], []
        for e in years:
            xVals.append(e)
            yVals.append(years[e])
        pylab.plot(xVals, yVals)
        pylab.xlabel('Year')
        pylab.ylabel('Mean Daily High (C)')
        pylab.title('Select U.S. Cites')
    fitting four different models(linear, quadratic, cubic, and quartic)
    initialize things:
        numSubsets = 10
        dimensions = (1, 2, 3, 4)
        rSquares = {}
        for d in dimensions:
            rSquares[d] = []
    split data:
        def splitData(xVals, yVals):
            toTrain = random.sample(range(len(xVals)), len(xVals)//2)
            trainX, trainY, testX, testY = [],[],[],[]
            for i in range(len(xVals)):
                if i in toTrain:
                    trainX.append(xVals[i])
                    trainY.append(yVals[i])
                else:
                    testX.append(xVals[i])
                    testY.append(yVals[i])
            return trainX, trainY, testX, testY
    random.sample(given this iterator(collection from 0 to n-1)select this many
        (in this case half "//2")of these numbers at random with no duplicates)
    train, test, and report:
        for f in range(numSubsets):
            trainX, trainY, testX, testY = splitData(xVals, yVals)
            for d in dimensions:
                model = pylab.polyfit(trainX, trainY, d)
                #estYVals = pylab.polyval(model, trainX)
                estYVals = pylab.polyval(model, testX)
                rSquares[d].append(rSquared(testY, estYVals))
        print('Mean R-squares for test data')
        for d in dimensions:
            mean = round(sum(rSquares[d])/len(rSquares[d]), 4)
            sd = round(numpy.std(rSquares[d]), 4)
            print('For dimensionality', d, 'mean =', mean, 'Std =', sd)
    results:
        mean r-squares for test data
        for dimensionality 1(linear)    mean = 0.7535 Std = 0.0656
        for dimensionality 2(quadratic) mean = 0.7291 Std = 0.0744
        for dimensionality 3(cubic)     mean = 0.7039 Std = 0.0684
        for dimensionality 4(quartic)   mean = 0.7169 Std = 0.0777

        linear seems to be the winner:
            highest average r-squared
            smallest deviation across trials
            simplest model
    why we should run multiple tests:
        note that deviations are a decimal order of magnitude smaller than means:
            suggests that while there is good agreement, deviations are large enough
            there could be a noticeable range of variation across trials
        suppose we had just run one trial:
            here are the R² values for each trial of linear fit:
                [0.7828002156420516, 0.80637964025052067,
                0.79637132757274265, 0.78433885743211906,
                0.76001112024853124, 0.57088936507035748,
                0.72115408562589023, 0.74358276762149023,
                0.79031455375148507, 0.77920238586399471]
            if we had only run one split, and happened to get this 0.57088936507035748
            result, we might have reached a different conclusion about validity of linear model

title subject definitions:
experimental data:
    collected through active intervention by the researcher
    to produce and measure change or to create difference
    when a variable is altered