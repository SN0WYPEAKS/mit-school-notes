lecture notes:
    linear spring:
        amount of force needed to stretch or compress spring
        is linear in the distance the spring is stretched
        or compressed
        each spring has a spring constant, k, that determines
        how much force is needed
    hooke's law of elasticity:
        the strain of the material is proportional to the
        applied stress within the elastic limit of that material
            F = -kd
            if a spring has a spring constant of 35,000 newtons
            how much does a rider have to weigh to compress
            the spring by one centimeter
            F=0.01m*35,000N/m
            F=350N
            F=mass*acc
            F=mass*9.8m per second squared which is the acceleration
            of gravity
            mass*9.8m/s2=350N
            mass=350N/9.81m/s2
            mass=350k/9.81 the k refers to kilograms, not the
            spring constant
            mass=35.68k or 79lbs
        there is a limit to the amount of force the law can hold
        before the spring breaks down
    finding k (spring constant):
        F=-kd       F = force
        k=-F/d      d = distance
        k=9.81*m/d  m = mass
    measuring distance:
        which should we choose?:
            x-axis, y-axis, or perpendicular to the line?
            Vertical distance because we want to predict
            dependent Y value for given independent X value,
            and vertical distance measures error in that prediction
    least squares objective function:
        representing the sum of squared differences between
        measurements stored in an experimental data file and a
        corresponding expression evaluated in the COMSOL Multiphysics model
        y = mx + b and y = a + bx
        In a least-squares regression for y = mx + b,
        m = N ∑ ( x y ) − ∑ x ∑ y N ∑ ( x 2 ) − ( ∑ x ) 2 and b = ∑ y − m ∑ x N,
        ∑:
            shorthand notation to indicate the sum of a number of similar terms.
        where N is the number of data points, while x and y are the
        coordinates of the data points.
        to minimize this objective function, i want to find a curve for the
        predicted observations that leads to minimum value
    linear regression:
        a data analysis technique that predicts the value of unknown data
        by using another related and known data value
    polynomials with one variable(x):
        0 or sum of finite number of non-zero terms
        each term of the form cxᵖ:
            c(constant), the coefficient, a real number
            p(power), the degree of the term, a non-negative integer
        the degree of the polynomial is the largest degree of any term
        examples:
            line: ax + b
            parabola: ax² + bx + c
    solving for least squares:
        len(observed)-1
        ∑ (observed[i]-predicted[i])²
        i=0
        simple example:
            use a degree-one polynomial, y=ax+b, as a model of our
            data(we want the best fitting line)
    polyfit:
        good news is that pylab provides built in functions to find these
        polynomial fits
        pylab.polyfit(observedX, observedY, n)
        finds coefficients of a polynomial of degree n, that provides 
        a best least squares fit for the observed data:
            n = 1 - best line       y = ax + b
            n = 2 - best parabola   y = ax**2 + bx + c
        it takes a collection of x values
        it takes a collection of equal length y values:
            they need to be equal length(assumed arrays)
        it takes an integer n which is the degree of fit that i want to apply
        n can be any non-negative integer
    polyval:
        evaluates the polynomial p at the points in x and
        returns the corresponding function values in y
    let's try a higher-degree model:
        model2 = pylab.polyfit(xVals, yVals, 2)
        pylab.plot(xVals, pylab.polyval(model2, xVals),
                   'r--', label = 'Quadratic Model')
        note that this is still an example of linear regression, even though
        we are not fitting a line to the data
        (in this case we are finding the best parabola) ax**2 + bx + c
        one axis is 'a' values
        second axis is 'b' values
        third axis is 'c' values
    comparing mean squared error:
        def aveMeanSquareError(data, predicted):
            error = 0.0
            for i in range(len(data)):
                error += (data[i] - predicted[i])**2
            return error/len(data)
        
        estYVals = pylab.polyval(model1, xVals)
        print('Ave. mean square error for linear model =',
              aveMeanSquareError(yVals, estYVals))
        estYVals = pylab.polyval(model2, xVals)
        print('Ave. mean square error for quadratic model =',
              aveMeanSquareError(yVals, estYVals))
        
        Ave. mean square error for linear model = 9372.73078965
        Ave. mean square error for quadratic model = 1524.02044718
        In this situation the quadratic is about six times better
        the residual error is about six times smaller
        how do i know if there is a better model out there
    the coefficent of determination:
        in an absolute sense:
            mean square error useful for comparing two different models
            for the same data
            useful for getting a sense of absolute goodness of fit?
            is 1524 good?
            hard to know, since there is no bound on values
            and not scale independent
            instead we use coefficent of determination, R²:
                R² = 1 - ∑ᵢ(yᵢ - pᵢ)²(Error in estimates)
                         -----------
                         ∑ᵢ(yᵢ - µ)²(variability in measured data)
                Yᵢ:
                    are measured values
                Pᵢ:
                    are predicted values
                µ:
                    is mean of measured values
                

title subject definitions:
    experimental data:
        collected through active intervention by the researcher
        to produce and measure change or to create difference
        when a variable is altered