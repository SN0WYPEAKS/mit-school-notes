lecture notes:
    the plan ahead:
        machine learning is a huge topic - with whole courses devoted to it:
            e.g., 6.008, 6.036, 6.860, 6.862, 6.867, and as central part
            of courses in natural language processing, computational biology,
            computer vision, robotics, other areas
        in 6.0002, we will:
            provide an introduction to the basic ideas, including ways to
            measure distances between examples, and how to group examples
            based on distance to create models
            introduce classification methods, such as "k nearest neighbor" methods
            introduce clustering methods, such as "k-means"

    what is machine learning:
        all useful programs learn something
        in the first lecture of 6.0001 we looked at an algorithm for finding square roots
        last week we looked at using linear regression to find a model of a collection of points
        early definition of machine learning:
            "field of study that gives computers the ability
            to learn without being explicitly programmed." Arthur Samuel (1959)
            computer pioneer who wrote first self-learning prgram, which played
            checkers - learned from "experience"
            invented alpha-beta pruning - widely used in decision tree searching
        traditional programming(example used: square root finder):
            you program the thing you want to do and input the data that you want
            the program to use then compare the output to the expected result
        machine learning(example used: curve fitting by linear regression):
            input the data that you want the program to use then input the
            output you expect. the machine will "learn"
            to build the program that you want.
    
    how are things learned?:
        memorization(declarative knowledge):
            accumulation of individual facts
            limited by:
                time to observe facts
                memory to store facts
        generalization(imperative knowledge):
            deduce new facts from old facts
            limited by accuracy of the deduction process:
                essentially a predictive activity
                assumes that the past predicts the future
    
    basic paradigm:
        observe set of examples:
            training data:
                spatial deviations relative to mass displacements of spring
        infer something about process that generated that data:
            fit polynomial curve using linear regression
        use inference to make predictions about previously unseen data:
            test data:
                predict displacements for other weights
        variations on paradigm:
            supervised:
                given a set of feature/label pairs, find a rule that
                predicts the label associated with a previously unseen input
            unsupervised:
                given a set of feature vectors (without labels) group them
                into "natural clusters" (or create labels for groups)

    some examples of classifying and clustering:
        here are some data on the new england patriots:
            name, height, weight
            labeled by type of position

        receivers:
            edelman     = ['edelman',    70, 200]
            hogan       = ['hogan',      73, 210]
            gronkowski  = ['gronkowski', 78, 265]
            amendola    = ['amendola',   71, 190]
            bennet      = ['bennet',     78, 275]
        
        linemen:
            cannon  = ['cannon', 77, 335]
            solder  = ['solder', 80, 325]
            mason   = ['mason',  73, 310]
            thuney  = ['thuney', 77, 305]
            karras  = ['karras', 76, 305]

    clustering examples into groups:
        want to decide on "similarity" of examples, with goal
        of separating into distinct, "natural", groups:
            similarity is a distance measure
        suppose we know that there are k different groups
        in our training data, but don't know labels (here k = 2):
            pick k samples (at random?) as exemplars
            cluster remaining samples by minimizing distance
            between samples in same cluster (objective function)
            - put sample in group with closest exemplar
            find median example in each cluster as new exemplar
            repeat until no change

    adding some new data:
        suppose we have learned to separate receivers versus linemen
        now ew are given some running backs, and want to use model to
        decide if they are more like receivers or linemen:
            blout = ['blout', 72, 250]
            white = ['white', 70, 205]

    machine learning methods:
        we will see some examples of machine learning methods:
            learn models based on unlabeled data, by clustering
            training data into group of nearby points:
                resulting clusters can assign labels to new data
            learn models that separate labeled groups of similar
            data from other groups:
                may not be possible to perfectly separate groups,
                without "over fitting"
                but can make decisions with respect to trading off
                "false positives" versus "false negatives"
                resulting classifiers can assign labels to new data

    feature representation:
        features never fully describe the situation:
            "all models are wrong, but some are useful." - George Box
        feature engineering:
            represent examples by feature vectors that will
            facilitate generalization
            suppose i want to use 100 examples from past to predict,
            at the start of the subject, which students will get an a in 6.0002
            some features surely helpful, e.g., GPA, prior programming
            experience (not a perfect predictor)
            other might cuase me to overfit, e.g., birth month, eye color
    maximize the features that carry the information we want
    and minimize those that don't

    measuring distance between animals:
        we can think of our animal examples as consisting of four binary features
        and one integer feature
        one way to learn to separate reptiles from non-reptiles is to measure
        the distance between pairs of examples, and use that:
            to cluster nearby examples into a common class (unlabeled data), or
            to find a classifier surface in space of examples that optimally
            separates different (labeled) collections of examples from other collections

        can convert examples into feature vectors:
            rattlesnake     = [1,1,1,1,0]
            boa constrictor = [0,1,0,1,0]
            dart frog       = [1,0,1,0,4]

    minkowski metric:
                        ₗₑₙ
        dist(X1,X2,p) = (Σabs(X1ₖ - X2ₖ)ᵖ)¹/ᵖ
                        ₖ₌₁
            p = 1: manhattan distance:
                the distance between two points measured along axes
                at right angles
            p = 2: euclidean distance:
                between two point in euclidean space is the length
                of the line segment between them

        need to measure distances between feature vectors
        typically use euclidean metric; manhattan may be appropriate
        if different dimensions are not comparable

    issues of concern when learning models:
        learned models will depend on:
            distance metric between examples
            choice of feature vectors
            constraints on complexity of model:
                specified number of clusters
                complexity of separating surface
                want to avoid over fitting problem
                (each example is its own cluster, or a complex separating surface)

    classification approaches:
        want to find boundaries in feature space that separate different classes
        of labeled examples:
            look for simple surface (e.g. best line or plane) that separates classes
            look for more complex surfaces (subject to constraints) that
            separate classes
            use voting schemes:
                find k nearest training examples, use majority vote to select label
        issues:
            how do we avoid over-fitting to data?
            how do we measure performance?
            how do we select best features?

    classification:
        attempt to minimize error on training data:
            similar to fitting a curve to data
        evaluate on training data

    other statistical measures:
                                                true positive
        positive predictive value(ppv) = ------------------------------
                                         true positive + false positive
        solid line model:
            .57
        dashed line model:
            .58
        complex model, training:
            .71
        complex model, testing:
            .78
        you will also see "sensitivity" versus "specificity"

                            true positive
        sensitivity = ------------------------------ (percentage correctly found)
                      true positive + false positive

                            true positive
        specificity = ------------------------------ (percentage correctly rejected)
                      true positive + false positive

title subject definitions:
machine learning:
    the use and development of computer systems that are able
    to learn and adapt without following explicit instructions,
    by using algorithms and statistical models to analyze and
    draw inferences from patterns in data.